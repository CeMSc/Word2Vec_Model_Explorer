{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "\n",
    "def read_data_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'\\W+', ' ', text.lower()).split()\n",
    "\n",
    "\n",
    "def train_word2vec_model(sentences):\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "    return model\n",
    "\n",
    "\n",
    "def find_related_words_and_bigrams(model, phraser, words_and_bigrams, top_n=30):\n",
    "    related_words_dict = {}\n",
    "    for word_or_bigram in words_and_bigrams:\n",
    "        if ' ' in word_or_bigram:\n",
    "            splitted = word_or_bigram.split(' ')\n",
    "            if phraser[splitted] == splitted:\n",
    "                print(f\"Bigram '{word_or_bigram}' not found in the model.\")\n",
    "                continue\n",
    "            else:\n",
    "                word_or_bigram = '_'.join(splitted)\n",
    "        try:\n",
    "            related_words = model.wv.most_similar(word_or_bigram, topn=top_n)\n",
    "            related_words_dict[word_or_bigram.replace('_', ' ')] = related_words\n",
    "        except KeyError:\n",
    "            print(f\"Word or bigram '{word_or_bigram.replace('_', ' ')}' not found in the model.\")\n",
    "    return related_words_dict\n",
    "\n",
    "\n",
    "def train_model_from_folder(folder_path):\n",
    "    texts = read_data_from_folder(folder_path)\n",
    "    sentences = [preprocess_text(text) for text in texts]\n",
    "\n",
    "    phrases = Phrases(sentences, min_count=1, threshold=10)\n",
    "    phraser = Phraser(phrases)\n",
    "    bigram_sentences = [phraser[sentence] for sentence in sentences]\n",
    "\n",
    "    model = train_word2vec_model(bigram_sentences)\n",
    "    return model, phraser\n",
    "\n",
    "\n",
    "def save_to_csv(related_words_dict, file_name='output.tsv'):\n",
    "    df = pd.DataFrame(related_words_dict)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: f\"{x[0].replace('_', ' ')}: {x[1]:.4f}\")\n",
    "    df.to_csv(file_name, index=False, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "def save_model(model, phraser, folder_path='./models'):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    model.save(os.path.join(folder_path, 'word2vec.model'))\n",
    "    with open(os.path.join(folder_path, 'phraser.pkl'), 'wb') as f:\n",
    "        pickle.dump(phraser, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[39mprint\u001b[39m()\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# ENTER THE FOLDER PATH WITH THE TXT FILES THAT YOU WANT TO USE TO TRAIN W2V\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     model, phraser \u001b[39m=\u001b[39m train_model_from_folder(folder_path)\n\u001b[0;32m      4\u001b[0m     save_model(model, phraser)\n\u001b[0;32m      6\u001b[0m     input_words \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# ENTER WORDS OR BIGRAMS SEPARATED BY COMMAS\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m, in \u001b[0;36mtrain_model_from_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model_from_folder\u001b[39m(folder_path):\n\u001b[1;32m---> 48\u001b[0m     texts \u001b[39m=\u001b[39m read_data_from_folder(folder_path)\n\u001b[0;32m     49\u001b[0m     sentences \u001b[39m=\u001b[39m [preprocess_text(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[0;32m     51\u001b[0m     phrases \u001b[39m=\u001b[39m Phrases(sentences, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mread_data_from_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_data_from_folder\u001b[39m(folder_path):\n\u001b[0;32m     11\u001b[0m     texts \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(folder_path):\n\u001b[0;32m     13\u001b[0m         \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     14\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, file), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    folder_path = \"\" # ENTER THE FOLDER PATH WITH THE TXT FILES THAT YOU WANT TO USE TO TRAIN W2V\n",
    "    model, phraser = train_model_from_folder(folder_path)\n",
    "    save_model(model, phraser)\n",
    "\n",
    "    input_words = (\"\").lower().strip().split(',') # ENTER WORDS OR BIGRAMS SEPARATED BY COMMAS\n",
    "    input_words = [word.strip() for word in input_words]\n",
    "\n",
    "    related_words_dict = find_related_words_and_bigrams(model, phraser, input_words)\n",
    "    save_to_csv(related_words_dict, file_name='output.tsv')\n",
    "        # Print related words\n",
    "    print(\"\\nRelated words and bigrams:\")\n",
    "    for key, value in related_words_dict.items():\n",
    "        print(f\"{key}:\")\n",
    "        for word, similarity in value:\n",
    "            print(f\"  {word.replace('_', ' ')}: {similarity:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
